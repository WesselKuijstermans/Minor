{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning (DQN) for Doom\n",
    "\n",
    "![](vizdoom.png)\n",
    "\n",
    "## Doom game rules, the BASIC scenario\n",
    "\n",
    "* The map is a rectangle with walls, ceiling and floor\n",
    "* A monster is spawned randomly somewhere along the opposite wall\n",
    "* The player can only go left/right or shoot\n",
    "* One hit is enough to kill the monster\n",
    "* Episode finishes when monster is killed or on timeout (300 tics).\n",
    "\n",
    "Rewards:\n",
    "* +100 for killing the monster\n",
    "* -1 for every time tick (every time tick there's an action left/right/shoot)\n",
    "* -5 missed shot\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "No specific reason for these specific versions, just that I used them and they worked. To prevent problems, you could you use these versions as well, although newer versions are available.\n",
    "\n",
    "* python 3.7\n",
    "* tensorflow 1.15\n",
    "* skimage latest version (conda install scikit-image)\n",
    "* vizdoom 1.1.7\n",
    "\n",
    "Installing vizdoom is easiest without using conda or pip:\n",
    "1. download version 1.1.7 from https://github.com/mwydmuch/ViZDoom/releases\n",
    "2. unpack the zip in `...\\Anaconda3\\envs\\<your-conda-env>\\Lib\\site-packages`\n",
    "3. you should now have a folder named `vizdoom` in the folder `site-packages`\n",
    "4. you will find a scenario folder in the vizdoom folder, copy basic.cfg and basic.wad which are inside the scenario folder and put them in the same folder as your own code. This step assumes you are trying to play the basic mission.\n",
    "5. if your python version doesn't match change version in file `...\\Anaconda3\\envs\\<your-conda-env>\\Lib\\site-packages\\vizdoom\\__init__.py`\n",
    "6. in basic.cfg (the version in the folder with your own code), change to \"screen_format = GRAY8\"\n",
    "7. activate the conda environment, open the python prompt and try:\n",
    "\n",
    "```\n",
    "    import vizdoom\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.init()\n",
    "```\n",
    "\n",
    "If a small graphical window opens, vizdoom installation is fine.\n",
    "\n",
    "Just a remark about the basic.cfg file. If you make a typo in a statement, there's no error message displayed. The statement is simply ignored by the parser of the file, leading to unexpected behavior. For example, in step 6, I first tried `screen_format = GRAY8  # used to be: CRCGCB` instead of `screen_format = GRAY8`. The parser can't handle comments behind a statement. The image was not single-channel grayscale, but remained 3-channel color, leading to the error `ValueError: ('Cannot warp empty image with dimensions', (0, 180, 320))`.\n",
    "\n",
    "More info on Vizdoom:\n",
    "* [vizdoom tutorial](http://vizdoom.cs.put.edu.pl/tutorial)\n",
    "* [vizdoom source code](https://github.com/mwydmuch/ViZDoom)\n",
    "\n",
    "## Training\n",
    "\n",
    "Training with 500 episodes takes about half an hour, however very much depending on type of computer and whether you use the GPU. The first couple of episodes take a lot of time, but as the agent improves, later episodes take less time to complete. Because the learned model is saved every 5 episodes, you can stop training before the 500 episodes have been done, without losing the training effort.\n",
    "\n",
    "Start tensorboard to see the loss decreasing:\n",
    "* tensorboard --logdir tensorboard_logs\n",
    "* http://localhost:6006/\n",
    "\n",
    "## Running\n",
    "\n",
    "After training the agent plays 100 episodes. Quite impressive to see the result in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: 100.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "Result: 95.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "Result: -380.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: 100.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "Result: 93.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from vizdoom import *  # Doom environment\n",
    "import random \n",
    "import time \n",
    "from skimage import transform\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\") \n",
    "    game.init()\n",
    "    \n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    actions = [left, right, shoot]\n",
    "    \n",
    "    return game, actions\n",
    "       \n",
    "def test_environment():\n",
    "    game, actions = create_environment()\n",
    "\n",
    "    episodes = 3\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            reward = game.make_action(action)\n",
    "            print(\"action: {}, reward: {}\".format(action, reward))\n",
    "            time.sleep(0.1)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()\n",
    "\n",
    "# just to playtest if the vizdoom environment works\n",
    "test_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wesse\\AppData\\Local\\Temp\\ipykernel_32304\\2983256963.py:161: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\wesse\\AppData\\Local\\Temp\\ipykernel_32304\\2983256963.py:70: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\wesse\\AppData\\Local\\Temp\\ipykernel_32304\\2983256963.py:73: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wesse\\AppData\\Local\\Temp\\ipykernel_32304\\2983256963.py\", line 162, in <module>\n",
      "    DQNetwork = DQNetwork(state_size, action_size, learning_rate)\n",
      "  File \"C:\\Users\\wesse\\AppData\\Local\\Temp\\ipykernel_32304\\2983256963.py\", line 86, in __init__\n",
      "    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\python\\util\\lazy_loader.py\", line 63, in __getattr__\n",
      "    return getattr(module, item)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\__init__.py\", line 39, in <module>\n",
      "    from tensorflow.contrib import compiler\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\__init__.py\", line 21, in <module>\n",
      "    from tensorflow.contrib.compiler import jit\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\__init__.py\", line 22, in <module>\n",
      "    from tensorflow.contrib.compiler import xla\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\xla.py\", line 22, in <module>\n",
      "    from tensorflow.python.estimator import model_fn as model_fn_lib\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\python\\estimator\\model_fn.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator import model_fn\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\model_fn.py\", line 29, in <module>\n",
      "    from tensorflow.python.types import core\n",
      "ModuleNotFoundError: No module named 'tensorflow.python.types'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wesse\\AppData\\Local\\Temp\\ipykernel_32304\\2983256963.py\", line 162, in <module>\n",
      "    DQNetwork = DQNetwork(state_size, action_size, learning_rate)\n",
      "  File \"C:\\Users\\wesse\\AppData\\Local\\Temp\\ipykernel_32304\\2983256963.py\", line 86, in __init__\n",
      "    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\python\\util\\lazy_loader.py\", line 63, in __getattr__\n",
      "    return getattr(module, item)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\__init__.py\", line 39, in <module>\n",
      "    from tensorflow.contrib import compiler\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\__init__.py\", line 21, in <module>\n",
      "    from tensorflow.contrib.compiler import jit\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\__init__.py\", line 22, in <module>\n",
      "    from tensorflow.contrib.compiler import xla\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\xla.py\", line 22, in <module>\n",
      "    from tensorflow.python.estimator import model_fn as model_fn_lib\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\python\\estimator\\model_fn.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator import model_fn\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\model_fn.py\", line 29, in <module>\n",
      "    from tensorflow.python.types import core\n",
      "ModuleNotFoundError: No module named 'tensorflow.python.types'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2080, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wesse\\AppData\\Local\\Temp\\ipykernel_32304\\2983256963.py\", line 162, in <module>\n",
      "    DQNetwork = DQNetwork(state_size, action_size, learning_rate)\n",
      "  File \"C:\\Users\\wesse\\AppData\\Local\\Temp\\ipykernel_32304\\2983256963.py\", line 86, in __init__\n",
      "    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\python\\util\\lazy_loader.py\", line 63, in __getattr__\n",
      "    return getattr(module, item)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\__init__.py\", line 39, in <module>\n",
      "    from tensorflow.contrib import compiler\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\__init__.py\", line 21, in <module>\n",
      "    from tensorflow.contrib.compiler import jit\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\__init__.py\", line 22, in <module>\n",
      "    from tensorflow.contrib.compiler import xla\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\xla.py\", line 22, in <module>\n",
      "    from tensorflow.python.estimator import model_fn as model_fn_lib\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_core\\python\\estimator\\model_fn.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator import model_fn\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\model_fn.py\", line 29, in <module>\n",
      "    from tensorflow.python.types import core\n",
      "ModuleNotFoundError: No module named 'tensorflow.python.types'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2080, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3396, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2080, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1143, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"c:\\Users\\wesse\\anaconda3\\envs\\doom\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "# normalize and resize the frame\n",
    "def preprocess_frame(frame):\n",
    "    # greyscale frame already done in basic.cfg: \"screen_format = GRAY8\"    \n",
    "    cropped_frame = frame[30:-10,30:-30]  # crop the screen (remove the roof because it contains no information)\n",
    "    normalized_frame = cropped_frame/255.0  # normalize pixel values\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])  # resize the frame to shape (84, 84)\n",
    "    \n",
    "    return preprocessed_frame\n",
    "\n",
    "\n",
    "# initialize deque with zero images; one array for each image\n",
    "stack_size = 4 # We stack 4 frames\n",
    "stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # clear stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # stack the frames; result is a tensor\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)  # resulting shape (84, 84, 4)\n",
    "        \n",
    "    else:\n",
    "        # append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # build the stacked state\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames\n",
    "\n",
    "\n",
    "game, possible_actions = create_environment()\n",
    "state_size = [84,84,4]  # input is a stack of 4 frames hence 84x84x4 (width, height, stackinglayers) \n",
    "action_size = game.get_available_buttons_size()  # 3 possible actions: left, right, shoot\n",
    "\n",
    "# learning parameters\n",
    "learning_rate =  0.0002 \n",
    "n_episodes = 500   \n",
    "max_steps = 100  # max possible steps in an episode\n",
    "batch_size = 64             \n",
    "gamma = 0.95  # discounting rate\n",
    "\n",
    "# exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# memory hyperparameters\n",
    "pretrain_length = batch_size   # number of experiences stored in the memory when initialized for the first time\n",
    "memory_size = 1000000          # number of experiences the memory can keep\n",
    "\n",
    "training = True\n",
    "\n",
    "\n",
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # create the placeholders\n",
    "            # *state_size means take each element of state_size in a tuple; like if we wrote [None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # remember that target_Q is the R(s,a) + y * max Q_hat(s', a')  (Q_hat is the estimated Q)\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\" First convnet: CNN, BatchNormalization, ELU \"\"\"\n",
    "            # input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            \n",
    "            \"\"\" Second convnet: CNN, BatchNormalization, ELU \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \n",
    "            \"\"\" Third convnet: CNN, BatchNormalization, ELU \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            \n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "\n",
    "  \n",
    "            # Q is the predicted Q-value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            \n",
    "            # the loss is the MSE of predicted Q_values and the Q_target\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            \n",
    "# instantiate the DQNetwork\n",
    "tf.reset_default_graph()\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]\n",
    "    \n",
    "# instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    \n",
    "    if i == 0:  # the first step\n",
    "        state = game.get_state().screen_buffer  # First we need a state\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    action = random.choice(possible_actions)  # random action\n",
    "    reward = game.make_action(action)\n",
    "    done = game.is_episode_finished()  # look if the episode is finished\n",
    "    \n",
    "    if done:  # we're dead\n",
    "        next_state = np.zeros(state.shape)  # we finished the episode\n",
    "        memory.add((state, action, reward, next_state, done))  # add experience to memory\n",
    "        \n",
    "        game.new_episode()  # start a new episode\n",
    "        state = game.get_state().screen_buffer  # first we need a state\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)  # Stack the frames\n",
    "        \n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer  # get the next state\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        \n",
    "        memory.add((state, action, reward, next_state, done))  # add experience to memory\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "writer = tf.summary.FileWriter(\"./tensorboard_logs/dqn\")  # setup TensorBoard Writer\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "write_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# the Q-learning part\n",
    "# choose action a from state s using epsilon greedy policy\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if(explore_probability > exp_exp_tradeoff):\n",
    "        action = random.choice(possible_actions)  # random action (exploration)\n",
    "        \n",
    "    else:\n",
    "        # get action from Q-network (exploitation)\n",
    "        # estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        choice = np.argmax(Qs)  # Take the biggest Q value (= the best action)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()  # will help to save our model\n",
    "\n",
    "if training:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0\n",
    "        game.init()\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            step = 0            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer  # observe the first state\n",
    "            \n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)  # stack_frames() also calls preprocess()\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                decay_step +=1\n",
    "                \n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                reward = game.make_action(action)\n",
    "                done = game.is_episode_finished()                \n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    step = max_steps  # set step = max_steps to end the episode\n",
    "\n",
    "                    total_reward = np.sum(episode_rewards)  # get the total reward of the episode\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Epsilon: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer  # get the next state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)  # stack the frame of the next_state\n",
    "                    memory.add((state, action, reward, next_state, done))  # add experience to memory\n",
    "                    state = next_state  # new state becomes current state\n",
    "\n",
    "\n",
    "                # learning part            \n",
    "                # obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                # get Q values for next_state \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # if we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "\n",
    "                # write TF summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished training the agent! Let the agent play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = create_environment()\n",
    "    n_episodes = 100\n",
    "    totalScore = 0\n",
    "    saver.restore(sess, \"./models/model.ckpt\")  # load the model\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        \n",
    "        done = False\n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            choice = np.argmax(Qs)  # greedy policy\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break                  \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
