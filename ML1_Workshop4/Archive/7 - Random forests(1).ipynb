{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Now let's grow a forest out of our trees. \n",
    "\n",
    "In this notebook we'll cover the following topics.\n",
    "- The problem of overfitting\n",
    "- Pruning\n",
    "- From a tree to a forest\n",
    "- A voting forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "We've seen the problem of overfitting in the previous notebook. There is a way to reverse overfitting (**regularization**) called **pruning**. Let's go through the steps of pruning a tree. (Pruning a tree is actually a horticultural expression used for cutting off branches to make the plant grow better in the future.)\n",
    "\n",
    "You need to have separated your data set into three parts for this. A **training set**, a **validation set**, and a **test set**. Once you have learned a tree on the training set you take the validation set and start cutting away at the tree: for example removing some of the leaf nodes. You then use the validation set to make sure that the tree does *not* lose any predictive power when cutting away these branches. If it does decrease an accurate prediction you're cutting away a crucial branch, so better put it back!\n",
    "\n",
    "Finally you can apply the tree to the test set for the final check that you have a model that is not overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forest\n",
    "\n",
    "A forest is also a method of **regularization**. Instead of cutting away branches that were overfitted you keep the overfitted tree, but simply generate lots of trees. Each tree is trained on a randomly sampled subset of the training data, so each tree is slightly different.\n",
    "\n",
    "And what do you call a set of many trees.... a **random forest**!\n",
    "\n",
    "When a prediction has to be made each tree is asked to go through its nodes and determine the outcome. This outcome is then cast as a vote by each tree. The forest looks at all the votes and sticks with the majority. The idea being that each tree will be overfitted differently than all the others, but the general trend is learned in all trees! Some trees will cast some ridiculous vote, but as long as they are in the minority the forest as a whole predicts correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Forest\n",
    "\n",
    "We'll start by importing the required modules and doing the same preprocessing of the data as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')\n",
    "df.sort_values(by=list(df.columns), axis=0, inplace=True)\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df = pd.get_dummies(df)\n",
    "df_train, df_test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "X_train = df_train.drop(columns=['survived', 'alive_no', 'alive_yes'])\n",
    "y_train = df_train['survived']\n",
    "X_test = df_test.drop(columns=['survived', 'alive_no', 'alive_yes'])\n",
    "y_test = df_test['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forest classifier is imported from the ensemble submodule of Scikit. Each tree is a separate model and we're combining their output through voting. This is called an **ensemble** of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the power of Scikit. The syntax when training a different model is almost identical!\n",
    "\n",
    "Let this inspire you to try other models available in Scikit: the interface to use them is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=50)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our trusty error to determine how this model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(truth, prediction):\n",
    "    diff = 0\n",
    "    \n",
    "    for truth_i, prediction_i in zip(truth, prediction):\n",
    "        diff += (truth_i - prediction_i)**2\n",
    "        \n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = forest.predict(X_test)\n",
    "pred_train = forest.predict(X_train)\n",
    "err_test = compute_error(y_test, pred_test)\n",
    "err_train = compute_error(y_train, pred_train)\n",
    "err_test, err_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... it doesn't improve much on this small data set, one tree is actually enough.\n",
    "\n",
    "But I challenge you to try it on the much larger Fraude Detection set! I bet you will see a change!\n",
    "\n",
    "---\n",
    "#### Exercise\n",
    "Did you try rerunning training the forest? Each time you train and test it you can get a slightly different number of mistakes. Can you explain why this is?\n",
    "\n",
    "---\n",
    "\n",
    "So far we have made precise predictions with both the decision tree and the random forest. Some models actually allow for giving a probability of being in each class. Let's try this for the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08      , 0.92      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.18      , 0.82      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.6       , 0.4       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9       , 0.1       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22      , 0.78      ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [0.9       , 0.1       ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.44      , 0.56      ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.82      , 0.18      ],\n",
       "       [0.52      , 0.48      ],\n",
       "       [0.82      , 0.18      ],\n",
       "       [0.86      , 0.14      ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.58      , 0.42      ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.76      , 0.24      ],\n",
       "       [0.78      , 0.22      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [0.84      , 0.16      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.68      , 0.32      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.8       , 0.2       ],\n",
       "       [0.28      , 0.72      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84      , 0.16      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.54      , 0.46      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.8       , 0.2       ],\n",
       "       [0.76      , 0.24      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26      , 0.74      ],\n",
       "       [0.9       , 0.1       ],\n",
       "       [0.58      , 0.42      ],\n",
       "       [0.26      , 0.74      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.12      , 0.88      ],\n",
       "       [0.32      , 0.68      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.68      , 0.32      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.62      , 0.38      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.64      , 0.36      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.86      , 0.14      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [0.78      , 0.22      ],\n",
       "       [0.08      , 0.92      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7       , 0.3       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82      , 0.18      ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84      , 0.16      ],\n",
       "       [0.54      , 0.46      ],\n",
       "       [0.8       , 0.2       ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.58      , 0.42      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.64      , 0.36      ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [0.44      , 0.56      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7       , 0.3       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.46      , 0.54      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [0.62      , 0.38      ],\n",
       "       [0.56      , 0.44      ],\n",
       "       [0.16      , 0.84      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22      , 0.78      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.72      , 0.28      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.56      , 0.44      ],\n",
       "       [0.52      , 0.48      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [0.52      , 0.48      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.56      , 0.44      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22      , 0.78      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.58      , 0.42      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [0.22      , 0.78      ],\n",
       "       [0.82      , 0.18      ],\n",
       "       [0.58      , 0.42      ],\n",
       "       [0.72      , 0.28      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.46      , 0.54      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [0.4       , 0.6       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76      , 0.24      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.62      , 0.38      ],\n",
       "       [0.8       , 0.2       ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.46      , 0.54      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.64      , 0.36      ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.78      , 0.22      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [0.26      , 0.74      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.62      , 0.38      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.24      , 0.76      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.78      , 0.22      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.26      , 0.74      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97666667, 0.02333333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.18      , 0.82      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24      , 0.76      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.3       , 0.7       ],\n",
       "       [0.95      , 0.05      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.28      , 0.72      ],\n",
       "       [0.26      , 0.74      ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.9       , 0.1       ],\n",
       "       [0.08      , 0.92      ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.6       , 0.4       ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.52      , 0.48      ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.62      , 0.38      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.86      , 0.14      ],\n",
       "       [0.54      , 0.46      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.86      , 0.14      ],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.14      , 0.86      ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.64      , 0.36      ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [0.08      , 0.92      ],\n",
       "       [0.28      , 0.72      ],\n",
       "       [0.42      , 0.58      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.56      , 0.44      ],\n",
       "       [0.32      , 0.68      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42      , 0.58      ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [0.14      , 0.86      ],\n",
       "       [0.1       , 0.9       ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [0.68      , 0.32      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.8       , 0.2       ],\n",
       "       [0.86      , 0.14      ],\n",
       "       [0.12      , 0.88      ],\n",
       "       [0.16      , 0.84      ],\n",
       "       [0.72      , 0.28      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.32      , 0.68      ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [0.58      , 0.42      ],\n",
       "       [0.14      , 0.86      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06      , 0.94      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9       , 0.1       ],\n",
       "       [0.56      , 0.44      ],\n",
       "       [0.2       , 0.8       ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14      , 0.86      ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.8       , 0.2       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.44      , 0.56      ],\n",
       "       [0.1       , 0.9       ],\n",
       "       [0.44      , 0.56      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [0.62      , 0.38      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.32      , 0.68      ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26      , 0.74      ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [0.66      , 0.34      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.52      , 0.48      ],\n",
       "       [0.58      , 0.42      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.38      , 0.62      ],\n",
       "       [0.76      , 0.24      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82      , 0.18      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.8       , 0.2       ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [0.16      , 0.84      ],\n",
       "       [0.24      , 0.76      ],\n",
       "       [0.1       , 0.9       ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04      , 0.96      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98      , 0.02      ],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.24      , 0.76      ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [0.9       , 0.1       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.86      , 0.14      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.36      , 0.64      ],\n",
       "       [0.56      , 0.44      ],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.02      , 0.98      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.64      , 0.36      ],\n",
       "       [0.42      , 0.58      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93333333, 0.06666667],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.26      , 0.74      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26      , 0.74      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88      , 0.12      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66      , 0.34      ],\n",
       "       [0.1       , 0.9       ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get two columns as prediction. The first column is the estimated probability that this passenger deceased, while the second is the probability that the passenger survived. You can also check that both probabilities sum to 1 for each passenger, since a passenger has either survived or not, there is no other outcome.\n",
    "\n",
    "When measuring the performance of a model giving probabilities can work in your advantage. It works well with the ROC curve measurement for example, hint hint... ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
